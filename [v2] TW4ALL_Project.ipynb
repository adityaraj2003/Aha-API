{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TW4ALL Project",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityaraj2003/Aha-API/blob/main/%5Bv2%5D%20TW4ALL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `⚙️ TW4ALL`\n"
      ],
      "metadata": {
        "id": "4Mm0bsoCIFNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <code>Install Requirements</code>\n",
        "!pip install PyGithub pycryptodome parse-torrent-title"
      ],
      "metadata": {
        "id": "0yazQh_ZYSa5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <code>Main Script</code>\n",
        "\n",
        "import PTN, json\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pyparsing import Word, nums, Optional, oneOf, Combine, Suppress, Group\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "import random\n",
        "import struct\n",
        "import base64\n",
        "from Crypto.Cipher import AES\n",
        "import codecs\n",
        "import re\n",
        "import json\n",
        "from http.cookiejar import CookieJar\n",
        "from six import BytesIO\n",
        "import subprocess\n",
        "from github import Github\n",
        "\n",
        "txt_file_location = 'urls.txt' # @param [\"urls1.txt\"] {allow-input: true}\n",
        "gdtot = True # @param [\"True\", \"False\"] {type:\"raw\"}\n",
        "filepress = True # @param [\"True\", \"False\"] {type:\"raw\"}\n",
        "slug = 'naruto-shippuden' # @param {type:\"string\"}\n",
        "title = 'Naruto Shippuden' # @param {type:\"string\"}\n",
        "repo_name = \"aditya76-git/tw4all-db\"\n",
        "class GD_SHARER_CONFIG:\n",
        "    GDTOT_DOMAIN = \"https://new2.gdtot.dad\"\n",
        "    GDTOT_API_KEY = \"49hNnTzadiZXeMILxAL3cmBYm1CvG\"\n",
        "    GDTOT_EMAIL = \"adityaraj7612@gmail.com\"\n",
        "\n",
        "    FILEPRESS_DOMAIN = \"https://new12.filepress.store\"\n",
        "    FILEPRESS_API_KEY = \"JlyyHsuYoD13XAbc7AZ6Rapdj/S6QImRwiPCsS6C6XY=\"\n",
        "\n",
        "class MegaClient:\n",
        "    API = \"https://g.api.mega.co.nz/cs\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.timeout = 160  # max time (secs) to wait for resp from api requests\n",
        "        self.sid = None\n",
        "        self.sequence_num = random.randint(0, 0xFFFFFFFF)\n",
        "        self.request_id = self.make_id(10)\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    @staticmethod\n",
        "    def make_id(length):\n",
        "        text = ''\n",
        "        possible = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
        "        for i in range(length):\n",
        "            text += random.choice(possible)\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def makebyte(x):\n",
        "        return codecs.latin_1_encode(x)[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def makestring(x):\n",
        "        return codecs.latin_1_decode(x)[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_url(url):\n",
        "        # parse file id and key from url\n",
        "        if '#' in url:\n",
        "            match = re.findall(r'/file/(.*)#(.*)', url)\n",
        "            if match:\n",
        "                file_id, key = match[0]\n",
        "                return file_id, key\n",
        "        return None, None\n",
        "\n",
        "    @staticmethod\n",
        "    def base64_url_decode(data):\n",
        "        data += '=='[(2 - len(data) * 3) % 4:]\n",
        "        for search, replace in (('-', '+'), ('_', '/'), (',', '')):\n",
        "            data = data.replace(search, replace)\n",
        "        return base64.b64decode(data)\n",
        "\n",
        "    @staticmethod\n",
        "    def a32_to_str(a):\n",
        "        return struct.pack('>%dI' % len(a), *a)\n",
        "\n",
        "    def aes_cbc_decrypt(self, data, key):\n",
        "      aes_cipher = AES.new(key, AES.MODE_CBC, self.makebyte('\\0' * 16))\n",
        "      return aes_cipher.decrypt(data)\n",
        "\n",
        "    def decrypt_attr(self, attr, key):\n",
        "        attr = self.aes_cbc_decrypt(attr, self.a32_to_str(key))\n",
        "        attr = self.makestring(attr)\n",
        "        attr = attr.rstrip('\\0')\n",
        "        return json.loads(attr[4:]) if attr[:6] == 'MEGA{\"' else False\n",
        "\n",
        "\n",
        "    def str_to_a32(self, b):\n",
        "        if isinstance(b, str):\n",
        "            b = self.makebyte(b)\n",
        "        if len(b) % 4:\n",
        "            # pad to multiple of 4\n",
        "            b += b'\\0' * (4 - len(b) % 4)\n",
        "        return struct.unpack('>%dI' % (len(b) / 4), b)\n",
        "\n",
        "    def _api_request(self, data, **kwargs):\n",
        "        params = {'id': self.sequence_num}\n",
        "        self.sequence_num += 1\n",
        "\n",
        "        if self.sid:\n",
        "            params.update({'sid': self.sid})\n",
        "\n",
        "        if kwargs:\n",
        "            params.update(kwargs)\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            data = [data]\n",
        "\n",
        "        req = self.session.post(\n",
        "            self.API,\n",
        "            params=params,\n",
        "            data=json.dumps(data),\n",
        "            timeout=self.timeout\n",
        "        )\n",
        "\n",
        "        json_resp = req.json()\n",
        "\n",
        "        if isinstance(json_resp, int):\n",
        "            raise Exception(\"API request error\", code=json_resp)\n",
        "\n",
        "        return json_resp[0]\n",
        "\n",
        "    def get_public_file_info(self, url):\n",
        "        file_handle, file_key = self._parse_url(url)\n",
        "\n",
        "        data = self._api_request({\n",
        "            'a': 'g',\n",
        "            'p': file_handle,\n",
        "            'ssm': 1\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if isinstance(data, int):\n",
        "            raise Exception(\"Error getting public file info\", code=data)\n",
        "\n",
        "        if 'at' not in data or 's' not in data:\n",
        "            raise ValueError(\"Unexpected result\", data)\n",
        "\n",
        "        #base64 to a32\n",
        "        key = self.str_to_a32(self.base64_url_decode(file_key))\n",
        "        k = (key[0] ^ key[4], key[1] ^ key[5], key[2] ^ key[6], key[3] ^ key[7])\n",
        "\n",
        "\n",
        "        size = data['s']\n",
        "        unencrypted_attrs = self.decrypt_attr(self.base64_url_decode(data['at']), k)\n",
        "\n",
        "        if not unencrypted_attrs:\n",
        "            return None\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'size': size,\n",
        "            'name': unencrypted_attrs['n']\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "class AppDriveClient:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def get_title(self, html_content):\n",
        "        return re.compile(r'<title>(.*?)</title>').search(html_content).group(1)\n",
        "\n",
        "\n",
        "    def get_size(self, html_content):\n",
        "        data = re.compile(r'Size\\s*:\\s*([\\d.]+)\\s*([KMGT]B)', re.IGNORECASE).search(html_content)\n",
        "        return \"{} {}\".format(data.group(1), data.group(2))\n",
        "\n",
        "    def get_public_file_info(self, url):\n",
        "        html_content = self.session.get(url).text\n",
        "        title = self.get_title(html_content)\n",
        "        size = self.get_size(html_content)\n",
        "\n",
        "        result = {\n",
        "            'name': title,\n",
        "            'size': basic_utils.convert_to_bytes(size)\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class GdriveClient:\n",
        "    API_FILE = \"https://www.googleapis.com/drive/v2/files/{}\"\n",
        "    API_KEY = \"AIzaSyBPO_VhHtvTL-gs35Nb24cSsjuxQasjlN0\"\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_gdrive_id(driveLink):\n",
        "        pattern = r\"(?<=/d/|id=)([\\w-]+)(?=&|\\?|$)\"\n",
        "\n",
        "        match = re.search(pattern, driveLink)\n",
        "\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            pattern = r\"/file/d/([\\w-]+)/\"\n",
        "            match = re.search(pattern, driveLink)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "\n",
        "    def get_public_file_info(self, url):\n",
        "        driveId = self.extract_gdrive_id(url)\n",
        "\n",
        "\n",
        "        response = self.session.get(\n",
        "            self.API_FILE.format(driveId),\n",
        "            params = {\n",
        "                \"supportsTeamDrives\" : \"true\",\n",
        "                \"key\" : self.API_KEY\n",
        "            }\n",
        "        ).json()\n",
        "\n",
        "\n",
        "\n",
        "        result = {\n",
        "            'size': response.get('quotaBytesUsed'),\n",
        "            'name': response.get('originalFilename'),\n",
        "            'created_date' : response.get('createdDate')\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class FileInfoExtractor:\n",
        "    def __init__(self):\n",
        "        self.gdriveclient = GdriveClient()\n",
        "        self.megaclient = MegaClient()\n",
        "        self.appdriveclient = AppDriveClient()\n",
        "\n",
        "    def get_data(self, url):\n",
        "        if \"mega.nz\" in url:\n",
        "            return self.megaclient.get_public_file_info(url)\n",
        "        elif \"drive.google.com\" in url:\n",
        "            return self.gdriveclient.get_public_file_info(url)\n",
        "        elif \"appdrive\" in url:\n",
        "            return self.appdriveclient.get_public_file_info(url)\n",
        "\n",
        "class BASIC_UTILS:\n",
        "    @staticmethod\n",
        "    def get_readable_time(seconds: int) -> str:\n",
        "        result = ''\n",
        "        (days, remainder) = divmod(seconds, 86400)\n",
        "        days = int(days)\n",
        "        if days != 0:\n",
        "            result += f'{days}d'\n",
        "        (hours, remainder) = divmod(remainder, 3600)\n",
        "        hours = int(hours)\n",
        "        if hours != 0:\n",
        "            result += f'{hours}h'\n",
        "        (minutes, seconds) = divmod(remainder, 60)\n",
        "        minutes = int(minutes)\n",
        "        if minutes != 0:\n",
        "            result += f'{minutes}m'\n",
        "        seconds = int(seconds)\n",
        "        result += f'{seconds}s'\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_gdrive_id(driveLink):\n",
        "        pattern = r\"(?<=/d/|id=)([\\w-]+)(?=&|\\?|$)\"\n",
        "\n",
        "        # Search for the pattern in the link\n",
        "        match = re.search(pattern, driveLink)\n",
        "\n",
        "        # If a match is found, return the file ID\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            # If no match is found with the first pattern, try another pattern\n",
        "            pattern = r\"/file/d/([\\w-]+)/\"\n",
        "            match = re.search(pattern, driveLink)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_progress_bar(percentage):\n",
        "        if not 0 <= percentage <= 100:\n",
        "            # Ensure the percentage is within the valid range\n",
        "            if percentage < 0:\n",
        "                percentage = 0\n",
        "            elif percentage > 100:\n",
        "                percentage = 100\n",
        "\n",
        "        filled_blocks = int(percentage / 100 * 20)\n",
        "        empty_blocks = 20 - filled_blocks\n",
        "\n",
        "        progress_bar = \"[\" + \"█\" * filled_blocks + \"░\" * empty_blocks + \"]\"\n",
        "\n",
        "        return progress_bar\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_netscape_cookies(cookie_data):\n",
        "        cookies_dict = {}\n",
        "        for line in cookie_data.splitlines():\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"#\") or not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) == 7:\n",
        "                domain, _, path, _, expiration, name, value = parts\n",
        "                cookies_dict[name] = value\n",
        "        return cookies_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def run_curl(url, headers=None, data=None, cookies=None):\n",
        "        try:\n",
        "            curl_command = ['curl', url]\n",
        "\n",
        "            if headers:\n",
        "                for header, value in headers.items():\n",
        "                    curl_command.extend(['-H', f'{header}: {value}'])\n",
        "\n",
        "            if cookies:\n",
        "                for cookie, value in cookies.items():\n",
        "                    curl_command.extend(['--cookie', f'{cookie}={value}'])\n",
        "\n",
        "            if data:\n",
        "                curl_command.extend(['--data-raw', data])\n",
        "\n",
        "            output = subprocess.check_output(curl_command)\n",
        "            output = output.decode('utf-8')\n",
        "            return output\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(\"Error:\", e)\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_to_bytes(size_str):\n",
        "        units = {'B': 1, 'KB': 1024, 'MB': 1024 ** 2, 'GB': 1024 ** 3, 'TB': 1024 ** 4}\n",
        "\n",
        "        size_str = size_str.strip().upper()\n",
        "        size_value = float(size_str[:-2])\n",
        "        size_unit = size_str[-2:]\n",
        "\n",
        "        if size_unit not in units:\n",
        "            raise ValueError(\"Invalid size unit\")\n",
        "\n",
        "        return int(size_value * units[size_unit])\n",
        "\n",
        "\n",
        "\n",
        "basic_utils = BASIC_UTILS()\n",
        "\n",
        "\n",
        "class GOOGLE_DRIVE_SHARER_UPLOADER:\n",
        "    FP_ADD_API = \"{}/api/v1/file/add\"\n",
        "    FP_FILE = \"{}/file/{}\"\n",
        "\n",
        "    GDTOT_ADD_FILE = \"{}/api/upload/link\"\n",
        "\n",
        "\n",
        "    def __init__(self, driveLink):\n",
        "        self.driveLink = driveLink\n",
        "        self.driveId = basic_utils.extract_gdrive_id(driveLink)\n",
        "\n",
        "    def to_filepress(self):\n",
        "\n",
        "        try:\n",
        "            headers = {\n",
        "              'Content-Type': 'application/json'\n",
        "            }\n",
        "\n",
        "            payload = json.dumps({\n",
        "              \"key\": GD_SHARER_CONFIG.FILEPRESS_API_KEY,\n",
        "              \"id\": self.driveId,\n",
        "              \"genre\": \"tv\",\n",
        "              \"isAutoUploadToStream\": True\n",
        "            })\n",
        "\n",
        "\n",
        "            response = requests.post(\n",
        "                self.FP_ADD_API.format(GD_SHARER_CONFIG.FILEPRESS_DOMAIN),\n",
        "                headers = headers,\n",
        "                data = payload\n",
        "            ).json()\n",
        "\n",
        "            return self.FP_FILE.format(GD_SHARER_CONFIG.FILEPRESS_DOMAIN , response['data']['_id'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"[+] Error {} {}\".format(self.driveLink, e))\n",
        "\n",
        "\n",
        "    def to_gdtot(self):\n",
        "\n",
        "\n",
        "          headers = {\n",
        "              'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
        "          }\n",
        "\n",
        "          payload = {\n",
        "            \"api_token\": GD_SHARER_CONFIG.GDTOT_API_KEY,\n",
        "            \"email\": GD_SHARER_CONFIG.GDTOT_EMAIL,\n",
        "            \"url\": \"https://drive.google.com/file/d/{}/view?usp=drive_link\".format(self.driveId)\n",
        "          }\n",
        "\n",
        "\n",
        "\n",
        "          response = requests.post(\n",
        "              self.GDTOT_ADD_FILE.format(GD_SHARER_CONFIG.GDTOT_DOMAIN),\n",
        "              headers = headers,\n",
        "              data = payload\n",
        "          )\n",
        "\n",
        "\n",
        "          return response.json()['data'][0]['url']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class URL_PROCESSOR:\n",
        "    def __init__(self, txt_file_location, to_filepress = True, to_gdtot = True):\n",
        "        self.txt_file_location = txt_file_location\n",
        "        self.to_filepress = to_filepress\n",
        "        self.to_gdtot = to_gdtot\n",
        "\n",
        "    def get_all_urls(self):\n",
        "\n",
        "        with open(self.txt_file_location, 'r') as file:\n",
        "            urls = [line.strip() for line in file if line.strip().startswith(('http://', 'https://'))]\n",
        "\n",
        "        self.urls = urls\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_host_name(url):\n",
        "        if \"mega.nz\" in url:\n",
        "            return \"MEGA\"\n",
        "        elif \"drive.google.com\" in url:\n",
        "            return \"Google Drive\"\n",
        "        elif \"appdrive\" in url:\n",
        "            return \"AppDrive\"\n",
        "        else:\n",
        "            return \"Unknown Host\"\n",
        "\n",
        "    @staticmethod\n",
        "    def ptn_parse(filename):\n",
        "        data = PTN.parse(filename)\n",
        "        new_data = {\n",
        "            \"title\" : data.get('title'),\n",
        "            \"resolution\" : data.get('resolution'),\n",
        "            \"season\" : data.get('season', 1),\n",
        "            \"episode\" : data.get('episode', 0),\n",
        "            \"is_hq\" : True if any(code in filename for code in [\"HQ\", \"hq\"]) else False,\n",
        "            \"bit_depth\" : 10 if any(code in filename for code in [\"10bits\", \"10bit\", \"10 bit\"]) else 8,\n",
        "            \"type\" : \"EPISODE\" if data.get('episode') and not data.get('year') else \"MOVIE\",\n",
        "            \"codec\" : \"H.264\" if any(code in filename for code in [\"x264\", \"H.264\", \"H264\", \"AVC\", \"avc\"]) else (\"H.265\" if any(code in filename for code in [\"x265\", \"H.265\", \"H265\", \"HEVC\", \"hevc\"]) else \"H.264\")\n",
        "        }\n",
        "\n",
        "        return new_data\n",
        "\n",
        "\n",
        "\n",
        "    def extract_info_from_url(self, url):\n",
        "        cloudfile = FileInfoExtractor()\n",
        "        data = cloudfile.get_data(url)\n",
        "        parsed_filename = self.ptn_parse(data['name'])\n",
        "        return {\n",
        "            'size': data['size'],\n",
        "            'title': parsed_filename['title'],\n",
        "            'resolution': parsed_filename['resolution'],\n",
        "            'season': parsed_filename['season'],\n",
        "            'episode': parsed_filename['episode'],\n",
        "            'is_hq': parsed_filename['is_hq'],\n",
        "            'bit_depth': parsed_filename['bit_depth'],\n",
        "            'codec': parsed_filename['codec'],\n",
        "        }\n",
        "\n",
        "    def process_url(self, url, encodes, episode_count):\n",
        "        host = self.get_host_name(url)\n",
        "        info = self.extract_info_from_url(url)\n",
        "\n",
        "        key = (info['title'], info['resolution'], info['season'], info['episode'], info['is_hq'], info['bit_depth'], info['codec'], info['size'])\n",
        "\n",
        "        if host == \"Google Drive\":\n",
        "            gdrive = GOOGLE_DRIVE_SHARER_UPLOADER(url)\n",
        "\n",
        "            if self.to_gdtot:\n",
        "                gdtot = gdrive.to_gdtot()\n",
        "\n",
        "                encodes[key].append({'host': \"GDTot\", 'link': gdtot})\n",
        "\n",
        "            if self.to_filepress:\n",
        "                filepress = gdrive.to_filepress()\n",
        "\n",
        "                encodes[key].append({'host': \"Filepress\", 'link': filepress})\n",
        "\n",
        "\n",
        "\n",
        "        encodes[key].append({'host': host, 'link': url})\n",
        "\n",
        "        processed_episodes = episode_count - threading.active_count()\n",
        "        progress_percentage = int((processed_episodes / episode_count) * 100) + 10\n",
        "\n",
        "        progress_percentage = progress_percentage == 100 if progress_percentage > 100 else progress_percentage\n",
        "\n",
        "        progress_bar = basic_utils.generate_progress_bar(progress_percentage)\n",
        "\n",
        "        print(f\"\\r[+] Progress: {progress_bar} {progress_percentage}%\", end='')\n",
        "\n",
        "    @staticmethod\n",
        "    def group_same_category_files_together(json_data):\n",
        "        grouped_files = defaultdict(list)\n",
        "\n",
        "        for encode in json_data[\"encodes\"]:\n",
        "            key = (\n",
        "                encode[\"type\"],\n",
        "                encode[\"size\"],\n",
        "                encode[\"resolution\"],\n",
        "                encode[\"season\"],\n",
        "                encode[\"episode\"],\n",
        "                encode[\"is_hq\"],\n",
        "                encode[\"bit_depth\"],\n",
        "                encode[\"codec\"],\n",
        "                encode[\"title\"],\n",
        "            )\n",
        "            grouped_files[key].extend(encode[\"files\"])\n",
        "\n",
        "        grouped_data = [{\n",
        "            \"type\" : contentType,\n",
        "            \"size\" : size,\n",
        "            \"resolution\": resolution,\n",
        "            \"season\": season,\n",
        "            \"episode\": episode,\n",
        "            \"is_hq\": is_hq,\n",
        "            \"bit_depth\": bit_depth,\n",
        "            \"codec\": codec,\n",
        "            \"title\": title,\n",
        "            \"files\": files\n",
        "        } for (contentType, size, resolution, season, episode, is_hq, bit_depth, codec, title), files in grouped_files.items()]\n",
        "\n",
        "        return grouped_data\n",
        "\n",
        "    def add_to_database(self, file_path):\n",
        "        g = Github(\"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\")\n",
        "        repo = g.get_repo(repo_name)\n",
        "\n",
        "        commit_message = \"{} Added\".format(\n",
        "            file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
        "        )\n",
        "\n",
        "        with open(file_path, \"r\") as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "        try:\n",
        "            contents = repo.get_contents(file_path)\n",
        "            repo.update_file(file_path, commit_message, file_content, contents.sha)\n",
        "        except Exception as e:\n",
        "            repo.create_file(file_path, commit_message, file_content)\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def break_all_files(data):\n",
        "        all_data = []\n",
        "\n",
        "        for entry in data:\n",
        "            for individual_file in entry['files']:\n",
        "                to_add_dict = {\n",
        "                    \"title\": title,\n",
        "                    \"slug\": slug,\n",
        "                    \"season\": entry.get('season'),\n",
        "                    \"episode\": entry.get('episode'),\n",
        "                    \"resolution\": entry.get('resolution'),\n",
        "                    \"is_hq\": entry.get('is_hq'),\n",
        "                    \"bit_depth\": entry.get('bit_depth'),\n",
        "                    \"codec\": entry.get('codec'),\n",
        "                    \"type\": entry.get('type'),\n",
        "                    \"size\": entry.get('size'),\n",
        "                    \"host\": individual_file.get('host'),\n",
        "                    \"link\": individual_file.get('link'),\n",
        "                }\n",
        "\n",
        "                all_data.append(to_add_dict)\n",
        "\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def restructure_data(self, data, season = None, episode = None):\n",
        "\n",
        "        data = [entry for entry in data if str(entry.get('season')) == str(season) and str(entry.get('episode')) == str(episode)]\n",
        "\n",
        "        restructured_data = {\n",
        "            \"data\": {\n",
        "                \"slug\": data[0][\"slug\"],\n",
        "                \"title\" : data[0][\"title\"],\n",
        "                \"season\": data[0][\"season\"],\n",
        "                \"episode\": data[0][\"episode\"],\n",
        "                \"type\": data[0][\"type\"],\n",
        "            },\n",
        "            \"encodes\": [],\n",
        "        }\n",
        "        encodes_dict = defaultdict(list)\n",
        "\n",
        "        for encode in data:\n",
        "            encode_key = (\n",
        "                encode[\"resolution\"],\n",
        "                encode[\"is_hq\"],\n",
        "                encode[\"bit_depth\"],\n",
        "                encode[\"codec\"],\n",
        "            )\n",
        "            encodes_dict[encode_key].append(encode)\n",
        "\n",
        "        for encode_key, encodes in encodes_dict.items():\n",
        "            encode_info = encodes[0]\n",
        "            encode_files = [\n",
        "                {\"host\": file[\"host\"], \"link\": file[\"link\"]} for file in encodes\n",
        "            ]\n",
        "\n",
        "            encode_info[\"files\"] = encode_files\n",
        "\n",
        "            del encode_info[\"host\"]\n",
        "            del encode_info[\"link\"]\n",
        "            del encode_info[\"season\"]\n",
        "            del encode_info[\"episode\"]\n",
        "            del encode_info[\"type\"]\n",
        "            del encode_info[\"title\"]\n",
        "            del encode_info[\"slug\"]\n",
        "\n",
        "            restructured_data[\"encodes\"].append(encode_info)\n",
        "\n",
        "\n",
        "        restructured_data = self.sort_data(restructured_data)\n",
        "\n",
        "        output_file_path = '{}-{}x{}.json'.format(data[-1][\"slug\"], data[-1][\"season\"], data[-1][\"episode\"])\n",
        "\n",
        "        with open(output_file_path, 'w') as outfile:\n",
        "            json.dump(restructured_data, outfile, indent=4)\n",
        "\n",
        "        self.add_to_database(output_file_path)\n",
        "\n",
        "        # os.remove(output_file_path)\n",
        "\n",
        "        return restructured_data\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_data(data):\n",
        "\n",
        "        def custom_sort(a):\n",
        "            resolution_order = {'480p': 0, '720p': 1, '1080p': 2}\n",
        "            codec_order = {'H.264': 0, 'H.265': 1}\n",
        "\n",
        "            return (\n",
        "                resolution_order[a['resolution']],\n",
        "                a['bit_depth'],\n",
        "                a['is_hq'],\n",
        "                codec_order[a['codec']]\n",
        "            )\n",
        "\n",
        "        data['encodes'].sort(key=custom_sort)\n",
        "\n",
        "        final = {\n",
        "            \"data\" : data['data'],\n",
        "            \"encodes\" : data['encodes']\n",
        "        }\n",
        "\n",
        "        return final\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_season_and_episodes(data):\n",
        "        season = set()\n",
        "        episode = set()\n",
        "\n",
        "        for entry in data:\n",
        "            season.add(entry.get('season'))\n",
        "            episode.add(entry.get('episode'))\n",
        "\n",
        "        return list(season), list(episode)\n",
        "\n",
        "\n",
        "    def process_each_season_and_episode(self, data):\n",
        "        seasons, episodes = self.extract_season_and_episodes(data)\n",
        "\n",
        "        total = len(seasons) * len(episodes)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        print(\"\\n[+] Update: Adding to Database\")\n",
        "\n",
        "        for season in seasons:\n",
        "            for episode in episodes:\n",
        "\n",
        "\n",
        "                self.restructure_data(data, season = season, episode = episode)\n",
        "\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        print(\"\\n[+] Update: Adding to Database Completed in {}\".format(basic_utils.get_readable_time(end - start)))\n",
        "\n",
        "    def start_process(self):\n",
        "\n",
        "        self.get_all_urls()\n",
        "\n",
        "        encodes = defaultdict(list)\n",
        "        threads = []\n",
        "        episode_count = len(self.urls)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        print(\"[+] Update: Sorting and Uploading to Google Drive Sharers\")\n",
        "\n",
        "\n",
        "        for url in self.urls:\n",
        "            thread = threading.Thread(target=self.process_url, args=(url, encodes, episode_count))\n",
        "            thread.start()\n",
        "            threads.append(thread)\n",
        "\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "\n",
        "\n",
        "        result = []\n",
        "\n",
        "        for key, files in encodes.items():\n",
        "            title, resolution, season, episode, is_hq, bit_depth, codec, size = key\n",
        "            result.append({\n",
        "                'files': files,\n",
        "                'title': title,\n",
        "                'resolution': resolution,\n",
        "                'season': season,\n",
        "                'episode': episode,\n",
        "                'is_hq': is_hq,\n",
        "                'bit_depth': bit_depth,\n",
        "                'codec': codec,\n",
        "                'size' : size,\n",
        "                'type': 'EPISODE'\n",
        "            })\n",
        "\n",
        "        final_data = {\n",
        "            'encodes': result\n",
        "        }\n",
        "\n",
        "        final_data = self.group_same_category_files_together(final_data)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        final_data = self.break_all_files(final_data)\n",
        "\n",
        "        self.process_each_season_and_episode(final_data)\n",
        "\n",
        "        print(\"\\n[+] Update: Sorting and Uploading Completed in {}\".format(basic_utils.get_readable_time(end - start)))\n",
        "\n",
        "\n",
        "URL_PROCESSOR(\n",
        "    txt_file_location,\n",
        "    to_filepress = filepress,\n",
        "    to_gdtot = gdtot\n",
        ").start_process()\n"
      ],
      "metadata": {
        "id": "ZcmHXhy-h7XN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e052a30-866a-426c-da9a-d87af1951d57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Update: Sorting and Uploading to Google Drive Sharers\n",
            "[+] Progress: [░░░░░░░░░░░░░░░░░░░░] False%\n",
            "[+] Update: Adding to Database\n",
            "\n",
            "[+] Update: Adding to Database Completed in 6s\n",
            "\n",
            "[+] Update: Sorting and Uploading Completed in 10s\n"
          ]
        }
      ]
    }
  ]
}